{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapers\n",
    "\n",
    "> This module contains multiple scrapers, mostly for NSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp scrapers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nbdev\n",
    "import random\n",
    "import zipfile\n",
    "import time\n",
    "import io\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "base_path = nbdev.config.get_config().lib_path\n",
    "nifty500_csv = base_path / \"../Data/Misc/ind_nifty500list.csv\"\n",
    "nifty100_csv = base_path / \"../Data/Misc/ind_nifty100list.csv\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_request_headers() -> dict:\n",
    "    user_agents_list = [\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"User-Agent\": random.choice(user_agents_list),\n",
    "        \"accept-language\": \"en,gu;q=0.9,hi;q=0.8\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bhavcopy data scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Function to save csv file from URL\n",
    "def save_csv_from_url(file_url, file_path):\n",
    "    print(f\"Downloading:{file_url}\")\n",
    "\n",
    "    headers = get_request_headers()\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        request = s.get(file_url, headers=headers, timeout=5)\n",
    "        cookies = dict(request.cookies)\n",
    "        r = s.get(file_url, headers=headers, timeout=5, cookies=cookies)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Status: {r.status_code}: {file_url}\")\n",
    "            print(f\"~~~~~~~~~~~~> Error downloading from: {file_url}\")\n",
    "            # raise Exception(f\"Error downloading file: {file_name}\")\n",
    "        else:\n",
    "            # Generate dataframe from bytes\n",
    "            df = pd.read_csv(io.BytesIO(r.content), encoding=\"ISO-8859-1\")\n",
    "\n",
    "            # Strip whitespace from column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "\n",
    "            # Strip whitespace from values\n",
    "            for c in df.columns:\n",
    "                if df[c].dtype == object:\n",
    "                    df[c] = df[c].str.strip()\n",
    "\n",
    "            # Save to csv\n",
    "            df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def save_daily_bhavcopy(d, random_delay=True):\n",
    "    # Get Year, Month, Day\n",
    "    year = d.year\n",
    "    month = d.strftime(\"%B\").upper()[:3]\n",
    "    day = d.date().strftime(\"%d\")\n",
    "\n",
    "    nse_url = \"https://archives.nseindia.com/content/historical/EQUITIES\"\n",
    "    nse_url_2 = \"https://www1.nseindia.com/content/historical/EQUITIES\"\n",
    "\n",
    "    # Fetches bhavcopy data for a given date, unzips and saves to local path\n",
    "\n",
    "    # Define paths\n",
    "    file_name = f\"cm{day:0>2}{month}{year}bhav.csv.zip\"\n",
    "    file_url = f\"{nse_url}/{year}/{month}/{file_name}\"\n",
    "    file_url_2 = f\"{nse_url_2}/{year}/{month}/{file_name}\"\n",
    "\n",
    "    zip_path = base_path / f\"../Data/Bhavcopy/Zips/{file_name}\"\n",
    "    unzip_dir = base_path / \"../Data/Bhavcopy/Raw\"\n",
    "\n",
    "    # If file exists, unzip and save to local path\n",
    "    if Path(zip_path).is_file():\n",
    "        print(f\"File {file_name} already exists.. unzipping\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "    else:\n",
    "        if random_delay:\n",
    "            time.sleep(random.randint(1, 10))\n",
    "\n",
    "        headers = get_request_headers()\n",
    "        # print(f\"Downloading:{file_url}\")\n",
    "\n",
    "        with requests.Session() as s:\n",
    "            r = s.get(file_url, allow_redirects=False, headers=headers)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "                print(f\"Status: {r.status_code}: {file_url}\")\n",
    "                r = s.get(file_url_2, allow_redirects=False, headers=headers)\n",
    "\n",
    "            if r.status_code != 200:\n",
    "                print(f\"Status: {r.status_code}: {file_url_2}\")\n",
    "                print(f\"~~~~~~~~~~~~> Error downloading file: {file_name}\")\n",
    "                # raise Exception(f\"Error downloading file: {file_name}\")\n",
    "            else:\n",
    "                open(zip_path, \"wb\").write(r.content)\n",
    "                with zipfile.ZipFile(io.BytesIO(r.content)) as zip_ref:\n",
    "                    zip_ref.extractall(unzip_dir)\n",
    "                    print(f\"Processed file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def fetch_bhavcopy_data_for_range(start_date, end_date):\n",
    "    # Get list of holidays\n",
    "    holidays = get_holidays_table()\n",
    "\n",
    "    # Get number of days between start and end date\n",
    "    n_days = (end_date - start_date).days\n",
    "    print(50 * \"-\")\n",
    "    print(f\"Fetching data for {n_days+1} days\")\n",
    "\n",
    "    \"\"\"Fetches bhavcopy data for a given date range and returns a pandas dataframe\"\"\"\n",
    "    for d in pd.date_range(start_date, end_date):\n",
    "        # Check if date is a NSE holiday\n",
    "        q = holidays[holidays.tradingDate.str.contains(d.strftime(\"%d-%b-%Y\"))]\n",
    "\n",
    "        # Check if date is a weekend\n",
    "        if d.weekday() > 4:\n",
    "            print(f\"Skipping {d.date()} as it is a weekend\")\n",
    "        # elif q.shape[0] > 0:\n",
    "        #     print(f\"Skipping {d.date()} as it is a holiday: {q.description.values[0]}\")\n",
    "        else:\n",
    "            save_daily_bhavcopy(d, random_delay=True)\n",
    "\n",
    "    print(\"Bhavcopy data download complete\")\n",
    "    print(50 * \"-\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc scrapers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of NSE Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def scrape_nse_holidays(segment: str) -> pd.DataFrame:\n",
    "    # sourcery skip: extract-method\n",
    "    holidays_url = \"https://www.nseindia.com/api/holiday-master?type=trading\"\n",
    "\n",
    "    # Segments for NSE holidays JSON\n",
    "    segments = {\n",
    "        \"CM\": \"Equities\",\n",
    "        \"MF\": \"Mutual Funds\",\n",
    "        \"SLBS\": \"Securities Lending &amp; Borrowing Schemes\",\n",
    "        \"FO\": \"Equity Derivatives\",\n",
    "        \"CD\": \"Currency Derivatives\",\n",
    "        \"COM\": \"Commodity Derivatives\",\n",
    "        \"IRD\": \"Interest Rate Derivatives\",\n",
    "        \"CBM\": \"Corporate Bonds\",\n",
    "        \"NDM\": \"New Debt Segment\",\n",
    "        \"NTRP\": \"Negotiated Trade Reporting Platform\",\n",
    "    }\n",
    "    segments = {v: k for k, v in segments.items()}\n",
    "\n",
    "    with requests.Session() as sess:\n",
    "        headers = get_request_headers()\n",
    "\n",
    "        r = sess.get(\"https://www.nseindia.com/\", headers=headers, timeout=5)\n",
    "        cookies = dict(r.cookies)\n",
    "        r = sess.get(holidays_url, headers=headers, timeout=5, cookies=cookies)\n",
    "\n",
    "        # Unpack json into a dataframe\n",
    "        tmp = json.loads(r.text)\n",
    "        return pd.DataFrame(tmp[segments[segment]])\n",
    "\n",
    "\n",
    "def get_holidays_table(segment: str = \"Equities\") -> pd.DataFrame:\n",
    "    holidays_path = Path(\n",
    "        base_path, \"../Data/Misc\", f\"Holidays_{datetime.now().year}.csv\"\n",
    "    )\n",
    "    if not holidays_path.exists():\n",
    "        # holidays_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        scrape_nse_holidays(segment).to_csv(holidays_path, index=False)\n",
    "\n",
    "    return pd.read_csv(holidays_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General scraper for list of stocks in various segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def scrape_symbol_list(file_url: str, file_path: str = None) -> List[str]:\n",
    "    print(f\"Downloading:{file_url}\")\n",
    "\n",
    "    headers = get_request_headers()\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        request = s.get(\"https://www.nseindia.com/\", headers=headers, timeout=5)\n",
    "        cookies = dict(request.cookies)\n",
    "        r = s.get(file_url, headers=headers, timeout=5, cookies=cookies)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Status: {r.status_code}: {file_url}\")\n",
    "            print(f\"~~~~~~~~~~~~> Error downloading from: {file_url}\")\n",
    "            # raise Exception(f\"Error downloading file: {file_name}\")\n",
    "        else:\n",
    "            # Parse json from response to dataframe\n",
    "            tmp = json.loads(r.text)\n",
    "            print(type(tmp))\n",
    "\n",
    "            if type(tmp) == dict:\n",
    "                if \"longterm\" in tmp.keys() and \"shortterm\" in tmp.keys():\n",
    "                    # Handles ASM list\n",
    "                    df_long = pd.DataFrame.from_dict(tmp[\"longterm\"][\"data\"])\n",
    "                    df_short = pd.DataFrame.from_dict(tmp[\"shortterm\"][\"data\"])\n",
    "                    df = pd.concat([df_long, df_short], axis=0)\n",
    "                else:\n",
    "                    # Handles insider list\n",
    "                    df = pd.DataFrame.from_dict(tmp[\"data\"])\n",
    "\n",
    "            # Handles GSM list\n",
    "            elif type(tmp) == list:\n",
    "                df = pd.DataFrame.from_dict(tmp)\n",
    "\n",
    "            if file_path:\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "            return df  # .symbol.to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of stocks in ASM and GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_asm_stocks() -> List[str]:\n",
    "    asm_url = \"https://www.nseindia.com/api/reportASM\"\n",
    "    asm_file_path = Path(base_path, \"../Data/Misc\", \"ASM_List.csv\")\n",
    "    return scrape_symbol_list(asm_url, asm_file_path)\n",
    "\n",
    "\n",
    "def get_gsm_stocks() -> List[str]:\n",
    "    gsm_url = \"https://www.nseindia.com/api/reportGSM\"\n",
    "    gsm_file_path = Path(base_path, \"../Data/Misc\", \"GSM_List.csv\")\n",
    "    return scrape_symbol_list(gsm_url, gsm_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Get list of illiquid stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_illiquid_stocks() -> List[str]:\n",
    "    illiquid_sheet = \"https://docs.google.com/spreadsheets/d/1xGjim5zIQbaP1oXm0EEIU28PMxkyFwW2ufyVoB-O-i8/export?gid=251665721&format=csv\"\n",
    "    df = pd.read_csv(illiquid_sheet)\n",
    "    return df.Symbol.to_list()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all stocks to be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_blocklist_stocks() -> List[str]:\n",
    "    gsm = get_gsm_stocks()\n",
    "    asm = get_asm_stocks()\n",
    "    illiquid = get_illiquid_stocks()\n",
    "    return list(set(asm + gsm + illiquid))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of stocks in insider trading info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_insider_trading_stocks() -> List[str]:\n",
    "    insider_url = \"https://www.nseindia.com/api/corporates-pit?index=equities\"\n",
    "    insider_file_path = Path(base_path, \"../Data/Misc\", \"Insider_List.csv\")\n",
    "    return scrape_symbol_list(insider_url, insider_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of stocks with Futures & Options derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_fno_stocks() -> List[str]:\n",
    "    fno_url = \"https://archives.nseindia.com/content/fo/fo_mktlots.csv\"\n",
    "    fno_file_path = Path(base_path, \"../Data/Misc\", \"FnO_List.csv\")\n",
    "    save_csv_from_url(fno_url, fno_file_path)\n",
    "    df = pd.read_csv(fno_file_path)\n",
    "\n",
    "    return [x for x in df.SYMBOL.str.strip().sort_values() if x != \"Symbol\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nifty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_nifty500_stocks():\n",
    "    # Get Nifty500 list\n",
    "    return pd.read_csv(nifty500_csv).Symbol.to_list()\n",
    "\n",
    "def get_nifty100_stocks():\n",
    "    # Get Nifty100 list\n",
    "    return pd.read_csv(nifty100_csv).Symbol.to_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of Symbol changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_symbol_change_list() -> List[str]:\n",
    "    symbol_change_url = (\n",
    "        \"https://archives.nseindia.com/content/equities/symbolchange.csv\"\n",
    "    )\n",
    "    symbol_change_file_path = Path(base_path, \"../Data/Misc\", \"Symbol_Changes.csv\")\n",
    "    save_csv_from_url(symbol_change_url, symbol_change_file_path)\n",
    "    df = (\n",
    "        pd.read_csv(symbol_change_file_path, header=None, parse_dates=[3])\n",
    "        .rename(\n",
    "            columns={0: \"Company_Name\", 1: \"Old_Symbol\", 2: \"New_Symbol\", 3: \"Date\"}\n",
    "        )\n",
    "        .query(\"Date > @pd.Timestamp(2013,1,1)\")\n",
    "        .sort_values(by=\"Date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return list(zip(df[\"Old_Symbol\"], df[\"New_Symbol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
